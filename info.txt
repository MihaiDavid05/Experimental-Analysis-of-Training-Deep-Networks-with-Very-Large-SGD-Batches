Write what happened with each config here!

config 1:
    For 250 epochs, batch_size 128 and init lr 0.1 it went smooth (I stopped it at 200).
    For 250 epochs, batch_size 2 and initial lr 0.1 got loss NaN !!!!
    If the first config worked, then I applied: minibatch_size = k * train_samples => k = 128 / 40000 = 0.0032 => new_lr = 0.1 * 0.0032 = 0.0003 (from second paper)
    But, it stopped due to early stopping (however it took waaay to long).

config2:
    Test accuracy 79,5 %. But training could have gone longer (fully run 100 ep).
    Try other network and training schedule.

config3: 300 ep instead of 100 BUT used early stopping (compared to 2)
    Stopped too early, around ep 70 !!!! Test accuracy was 78,5 %

config4: Without earlystopping and with batchnorm momentum of 0.99 (compared to 3)
    I stopped it at 280. Test accuracy was 81.08 ! Last 2 best models at epochs 140 and 244 !

config5: Added the weight_decay and made steplr scheduler configurable and added new weight initialisations to network and changed rotation to 15
    Test accuracy is 80.8 ! Best epoch was 133 !



Papers summary:

***** https://arxiv.org/pdf/1804.07612.pdf

 - Use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide
improved generalization performance and allows a significantly smaller memory footprint
 - We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation),
and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size m.
 - The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size
progressively reduces the range of learning rates that provide stable convergence and acceptable test performance.
 - The best performance has been consistently obtained for mini-batch sizes between m = 2 and m = 32,
which contrasts with recent work advocating the use of mini-batch sizes in the thousands.
 - In this work, we suggest that the discussion about the scaling of the learning rate with the batch size depends on how
the problem is formalized, and revert to the view of Wilson & Martinez (2003), that considers the SGD weight update formulation based
on the sum, instead of the average, of the gradients over the mini-batch. (NOTE: reduction type in CrossEntropyLoss)
 - From this perspective, using a fixed learning rate keeps the expectation of the weight update per training example constant for
any choice of batch size.  At the same time holding the expected value of the weight update per gradient calculation
constant while increasing the batch size implies a linear increase of the variance of the weight update with the batch size.
 -  The results provide evidence that increasing the batch size results in both a
degradation of the test performance and a progressively smaller range of learning rates that allows
stable training, where the notion of stability here refers to the robust convergence of the training algorithm
 - Hoffer et al. (2017) have shown empirically that the reduced generalization performance of large
batch training is connected to the reduced number of parameter updates over the same number of
epochs (which corresponds to the same computation cost in number of gradient calculations). Hoffer
et al. (2017) present evidence that it is possible to achieve the same generalization performance with
large batch size, by increasing the training duration to perform the same number of SGD updates.
Since from (3) or (5) the number of gradient calculations per parameter update is proportional to the
batch size m, this implies an increase in computation proportional to m.
 - In order to reduce the computation cost for large batches due to the required longer training, Goyal
et al. (2017) have investigated the possibility of limiting the training length to the same number
of epochs. To this end, they have used larger learning rates with larger batch sizes based on the
linear scaling rule, which from (5) is equivalent to keeping constant the value of η˜. As discussed
in Section 2.2, this implies an increase of the variance of the weight update linearly with the batch
size. The large batch training results of Goyal et al. (2017) report a reduction in generalization
performance using the linear scaling rule with batch sizes up to 8192. However, Goyal et al. (2017)
have found that the test performance of the small batch (m = 256) baseline could be recovered using
a gradual warm-up strategy. We explore the impact of this strategy in Section 3.4 and find that it
does not fully mitigate the degradation of the generalization performance with increased batch size.
 - Jastrzebski et al. (2017) claim that both the SGD generalization performance and training dynamics
are controlled by a noise factor given by the ratio between the learning rate and the batch size,
which corresponds to linearly scaling the learning rate. The paper also suggests that the invariance
to the simultaneous rescaling of both the learning rate and the batch size breaks if the learning rate
becomes too large or the batch size becomes too small. However, the evidence presented in this
paper only shows that the linear scaling rule breaks when it is applied to progressively larger batch
sizes.
 - It has been shown that the optimum momentum coefficient is dependent on the batch size (Smith & Le, 2017; Goyal et al.,
2017). For this reason momentum was not used, with the purpose of isolating the interaction between batch size and learning rate.
 - For all learning rates and all batch sizes, training has been performed over the same number of epochs, which corresponds to a constant
computational complexity (a constant number of gradient calculations).
 - Unless stated otherwise, all the experiments have been run for a total of 82 epochs for CIFAR-10 and
164 epochs for CIFAR-100, with a learning rate schedule based on a reduction by a factor of 10 at 50% and at 75% of
the total number of iterations. Weight decay has been also applied, with λ = 5 · 10−4 for the AlexNet model (Krizhevsky et al., 2012)
and λ = 10−4 for the ResNet models (He et al., 2015a). In all cases, the weights have been initialized using the
method described in He et al. (2015b).
 - For all the results, the reported test or validation accuracy is the median of the final 5 epochs of
training (Goyal et al., 2017).
 - .................

 !!!!  the use of a gradual warm-up did improve the performance of the large batch sizes, but did not fully recover the
performance achieved with the smallest batch sizes. That's why we used cyclicLR !!!!!!


*** https://arxiv.org/pdf/1706.02677.pdf

 - The per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size.
 - We show that on the ImageNet dataset large minibatches cause optimization difficulties, but when
these are addressed the trained networksexhibit good generalization
 - We show no loss of accuracy when training with large minibatch sizes up to 8192 images. We adopt a
hyperparameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new
warmup scheme that overcomes optimization challenges early in training.
 - With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192
on 256 GPUs in one hour, while matching small minibatch accuracy
 - Optimization difficulty is the main issue with large minibatches, rather than poor generalization (at least on ImageNet)
 - !!!!! LINEAR SCALING RULE: When the minibatch size is multiplied by k, multiply the learning rate by k !!!!!!
 - However, there are at least two cases when the condition ∇l(x, wt) ≈ ∇l(x, wt+j ) will clearly not hold.
First, in initial training when the network is changing rapidly, it does not hold. We address this by using a warmup phase.
 - Second, minibatch size cannot be scaled indefinitely: while results are stable for a large range of sizes,
beyond a certain point accuracy degrades rapidly. Interestingly, this point is as large as ∼8k in ImageNet experiments.
 -  Krizhevsky reported a 1% increase of error when increasing the minibatch size from 128 to 1024, whereas we show how to maintain
accuracy across a much broader regime of minibatch sizes.
 - Li [25] (§4.6) showed distributed ImageNet training with minibatches up to 5120 without a loss in accuracy after
convergence. However, their work did not demonstrate a hyper-parameter search-free rule for adjusting the learning rate
as a function of minibatch size, which is a central contribution of our work.
 - Bottou et al suggest the learning rate should not exceed a maximum rate independent of minibatch size (which justifies warmup).
 - Constant warmup: Uses a low constant learning rate for the first few epochs of training
 - Gradual warmup: Gradually ramps up the learning rate from a small to a large value. This ramp avoids a sudden increase
of the learning rate, allowing healthy convergence at the start of training. In practice, with a large minibatch of size kn,
we start from a learning rate of η and increment it by a constant amount at each iteration such that it reaches
ηˆ = kη after 5 epochs (results are robust to the exact duration of warmup). After the warmup, we go back to the original learning rate schedule.
 - REMARK: Scaling the cross-entropy loss is not equivalent to scaling the learning rate.
 - !!!!!!! We refer to the factor ηt+1/ηt as the momentum correction !!!!!!!
 - REMARK: Apply momentum correction after changing learning rate.
 - REMARK: Use a single random shuffling of the training data (per epoch) that is divided amongst all k workers.
 - Applying the linear scaling rule along with a warmup strategy allows us to seamlessly scale between small and large
minibatches (up to 8k images) without tuning additional hyper-parameters or impacting accuracy.
 - EXPERIMENTS: All models are trained for 90 epochs regardless of minibatch sizes. We apply the linear scaling rule from §2.1 and
use a learning rate of η = 0.1 * kn/256 that is linear in the minibatch size kn. With k = 8 workers (GPUs) and n = 32
samples per worker, η = 0.1 as in [16]. We call this number (0.1 * kn /256) the reference learning rate, and reduce it by
1/10 at the 30-th, 60-th, and 80-th epoch, similar to [16].
 - Our goal is to match errors across minibatch sizes by using a general strategy that avoids hyper-parameter tuning
for each minibatch size.
 - A constant warmup strategy actually degrades results.
 - Our main result is that with gradual warmup, large minibatch training error matches the baseline training curve obtained
with small minibatches.
 - !!!!! The comparison between no warmup and gradual warmup suggests that large minibatch sizes are challenged by optimization
difficulties in early training and if these difficulties are addressed, the training error and its curve can match a
small minibatch baseline closely.
 - !!!! If the optimization issues are addressed, there is no apparent generalization degradation observed using large minibatch training
 - For all models we used the linear scaling rule and set the reference learning rate as η = 0.1 * kn/256. For models with kn > 256, we used
the gradual warmup strategy always starting with η = 0.1 and increasing linearly to the reference learning rate after 5 epochs
 - They showed that multiplying LR with sqrt(k) works poor in practice.


** https://arxiv.org/pdf/1404.5997.pdf

 - We can make data parallelism arbitrarily efficient if
we are willing to increase the batch size (because the weight synchronization step is performed once per
batch). But very big batch sizes adversely affect the rate at which SGD converges as well as the quality of
the final solution.
 - As we take the effective batch size, 128K, into the thousands, using a smaller
batch size in the fully-connected layers leads to faster convergence to better minima.
 - The first question that I investigate is the accuracy cost of larger batch sizes. This is a somewhat
complicated question because the answer is dataset-dependent. Small, relatively homogeneous datasets
benefit from smaller batch sizes more so than large, heterogeneous, noisy datasets.
 - When experimenting with different batch sizes, one must decide how to adjust the hyperparameters
µ, ω, and epsilon. It seems plausible that the smoothing effects of momentum may be less necessary with bigger
batch sizes, but in my experiments I used µ = 0.9
for all batch sizes. Theory suggests that when multiplying the batch size by k, one should multiply the
learning rate epsilon by √k to keep the variance in the gradient expectation constant.
 - Theory aside, for the batch sizes considered in this note, the heuristic that I found to work the
best was to multiply the learning rate by k when multiplying the batch size by k. I can’t explain this
discrepancy between theory and practice.
 - The main take-away is that there is an accuracy cost associated with bigger batch sizes, but it can be greatly reduced
by using the variable batch size trick described in Section 4.2.
 - We can expect some loss of accuracy when training with bigger batch sizes. The magnitude of this loss
is dataset-dependent, and it is generally smaller for larger, more varied datasets


* https://proceedings.neurips.cc/paper/2019/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf

 - Empirical evidence of a training strategy that we should control the ratio of batch size to learning rate not too
large to achieve a good generalization ability
 - !!!!! We only consider the case that the batch size and learning rate are constant !!!!
 - Batch size and learning rate, adjust the fluctuation of gradient
 - [30] proves that for linear neural networks, batch size has an optimal value when the learning rate is fixed
 - [29] suggests that increasing the batch size during the training of neural networks can receive the same result
of decaying the learning rate when the rate of batch size to learning rate remains the same. This result is consistent with ours.


* https://www.pure.ed.ac.uk/ws/portalfiles/portal/75846467/width_of_minima_reached_by_stochastic_gradient_descent_is_influenced_by_learning_rate_to_batch_size_ratio.pdf

 - We show that the dynamics and convergence properties of SGD are set by the ratio of learning rate to batch size
 - We observe that this ratio is a key determinant of the generalization error, which we suggest is mediated by
controlling the width of the final minima found by SGD.
 - SGD performs similarly for different batch sizes but a constant LR/BS
 - We derive a relation between LR/BS and the width of the minimum found by SGD
 - We demonstrate experimentally that a larger LR/BS correlates with a wider endpoint of SGD and better generalization.
 - There is a theoretical relationship between the expected loss value, the level of stochastic noise η/S in SGD, and the width of the
minimum explored at this final stage of training
 - Demonstrating that the learning dynamics are approximately invariant under changes in learning rate or batch size that
keep the ratio η/S constant.
 - Higher values of η/S push the optimization towards flatter regions
 - Smaller datasets more sensitive to LR/BS ratio.


* https://fid3024.github.io/papers/2020%20-%20Don't%20Use%20Large%20Mini-Batches,%20Use%20Local%20SGD.pdf

 - Models trained with large batches often do not generalize well, i.e. they do not show good accuracy on new data
 - We propose a post-local SGD and show that it significantly improves the generalization performance compared to
large-batch training on standard benchmarks while enjoying the same efficiency (time-to-accuracy) and scalability
 - !! Oriented on distributed learning !!


* Hoffer: https://arxiv.org/pdf/1705.08741.pdf

 - Fixed learning rate that decreases exponentially every few epochs
 - Previous work by Keskar et al:
    - Training models with large batch size increase the generalization error.
    This "generalization gap" seemed to remain even when the models were trained without limits, until the loss function ceased to improve.
    - Low generalization was correlated with "sharp" minima. Good generalization was correlated with "flat" minima.
 - We "stretched" the time-frame of the optimization process, where each time period of e epochs in the original regime,
will be transformed to |BL|/|BS| * e epochs according to the mini-batch size used.
 - ηL = sqrt(|BL|/|BS|) * ηS where ηS is the original learning rate used for small batch, ηL
is the adapted learning rate and |BL| - 4096, |BS| - 128 are the large and small batch sizes, respectively.
 - Ghost batch size is 128.


NOTE: Focus on generalization power of large batches, instead of time efficiency, because we have a single GPU.


EXPERIMENTS RESULTS

Improvements: Maybe use gradient clipping for high batch sizes ?!

Exp1:

    config1: 86.37 test acc ; 51.30 sec avg time / epoch
    config2: 85.08 test acc ; 48.20 sec avg time / epoch
    config3: 82.37 test acc ; 41.33 sec avg time / epoch
    config4: 74.40 test acc ; 41.25 sec avg time / epoch
    config5: 59.12 test acc ; 40.77 sec avg time / epoch
    config6: 39.46 test acc ; 38.46 sec avg time / epoch

Exp2:

    config1: 86.58 test acc
    config2: 85.97 test acc
    config3: 85.35 test acc
    config4: 82.79 test acc
    config5: ????? test acc

Exp3:
    For batch_size >= 512, the scaling rule + GhostBN does not help anymore.

    config1: 88.22 test acc ; 51.40 sec avg time / epoch
    config2: 88.51 test acc ; 48.31 sec avg time / epoch
    config3: 88.44 test acc ; 40.81 sec avg time / epoch
    config4: 88.18 test acc ; 40.81 sec avg time / epoch
    config5: 81.29 test acc ; 39.22 sec avg time / epoch
    config6: 79.07 test acc ; 34.04 sec avg time / epoch

Exp4:

    config1: 87.91 test acc ; 49.71 sec avg time / epoch
    config2: 88.25 test acc ; 45.73 sec avg time / epoch
    config3: 87.79 test acc ; 33.09 sec avg time / epoch
    config4: 87.85 test acc ; 32.30 sec avg time / epoch
    config5: 87.27 test acc ; 32.09 sec avg time / epoch
    config6: 85.97 test acc ; 33.39 sec avg time / epoch

Exp5:

    config1: 90.98 test acc ; 59.38 sec avg time / epoch
    config2: 91.32 test acc ; 44.55 sec avg time / epoch
    config3: 90.78 test acc ; 49.99 sec avg time / epoch
    config4: 90.78 test acc ; 42.83 sec avg time / epoch
    config5: 90.23 test acc ; 42.67 sec avg time / epoch

    config6: 82.79 test acc ; 42.40 sec avg time / epoch ----> with lr_factor 10
    config61: 88.61 test acc                             ----> with lr_factor 5
    config62: 89.87 test acc                             ----> with lr_factor 2
    config63: 89.46 test acc                             ----> just triangular with l_+factor 2